{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Chris\n",
    "\n",
    "The goal is to create a chatbot, given a variable length input sequence to output a variable length output that sounds like Chris is writing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.5.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "# keras with tensorflow backend\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.layers import Flatten, RepeatVector, TimeDistributed, CuDNNLSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "% matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "mpl.rcParams['figure.dpi']= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "txt_files = glob.glob(os.path.join(data_folder, '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    corr = re.sub('[^A-Za-z0-9öüäéè ]+', '', name)\n",
    "    if (corr[-1] == ' '):\n",
    "        corr = corr[:-1]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_txt_file(file_name):\n",
    "    f = open(file_name)\n",
    "    file_name = os.path.split(file_name)[1]\n",
    "    \n",
    "    date = []\n",
    "    time = []\n",
    "    sender = []\n",
    "    reciever = []\n",
    "    message = []\n",
    "\n",
    "    partner_name = file_name[18:-4] + \": \"\n",
    "    chris_name = 'Christoph Bernkopf: '\n",
    "\n",
    "    for line in f:\n",
    "        if(len(line) < 2 or \"Nachrichten in diesem Chat sowie Anrufe\" in line):\n",
    "    #         print(\"skip line\")\n",
    "            continue\n",
    "        if(line[15:18] == \" - \" and line[2] == \".\" and line[5] == \".\"):\n",
    "            date.append(line[:8])\n",
    "            time.append(line[10:15])\n",
    "            if (chris_name in line[18:]):\n",
    "                sender.append(clean_name(chris_name[:-2]))\n",
    "                reciever.append(clean_name(partner_name[:-2]))\n",
    "            else:\n",
    "                reciever.append(clean_name(chris_name[:-2]))\n",
    "                sender.append(clean_name(partner_name[:-2]))\n",
    "            message.append(line[18:].replace(partner_name, '').replace(chris_name, ''))\n",
    "\n",
    "    df = pd.DataFrame(date, columns=['date'])\n",
    "    df['time'] = time\n",
    "    df['sender'] = sender\n",
    "    df['reciever'] = reciever\n",
    "    df['message'] = message\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del df\n",
    "except:\n",
    "    None\n",
    "\n",
    "for file_name in txt_files:\n",
    "    print(file_name)\n",
    "    curr_df = get_df_from_txt_file(file_name)\n",
    "    if ('df' in locals()):\n",
    "        df = pd.concat([df, curr_df])\n",
    "    else:\n",
    "        df = curr_df\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(curr_df.shape, df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Messages into Inputs and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = ['<Medien ausgeschlossen>', '\\n', 'Martin TPH CERN <3: ', 'Christoph Gerhardus :): ', 'Megan :): ', 'Standort: ']\n",
    "\n",
    "for item in remove_list:\n",
    "    df['message'] = df['message'].apply(lambda x: x.replace(item, ''))\n",
    "\n",
    "# df['message'] = df['message'][df['message'].astype('str').str[:10] != \"Standort: \"]\n",
    "\n",
    "# get rid of links\n",
    "df['message'] = df['message'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# get rid of nan messages\n",
    "df['message'] = df['message'].apply(lambda x : x if type(x) == str else \"\")\n",
    "\n",
    "df = df[df['message'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    if (type(x) != str):\n",
    "        print(x)\n",
    "        \n",
    "df['message'].apply(lambda x: func(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_is_emoji(character):\n",
    "    return character in emoji.UNICODE_EMOJI\n",
    "\n",
    "def char_is_spe_char(character):\n",
    "    return character in \"?!.\"\n",
    "\n",
    "def text_has_emoji(text):\n",
    "    for character in text:\n",
    "        if character in emoji.UNICODE_EMOJI:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def text_has_spe_char(text):\n",
    "    for character in text:\n",
    "        if character in \"?!.\":\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_emoijis(x):\n",
    "    if (text_has_emoji(x) or text_has_spe_char(x)):\n",
    "        result = ''\n",
    "        if (len(x) == 1):\n",
    "            return result\n",
    "        for ch in x:\n",
    "            if (char_is_emoji(ch)):\n",
    "                result = result + ' ' + ch\n",
    "            elif (char_is_spe_char(ch)):\n",
    "                result = result + ' ' + ch\n",
    "            else:\n",
    "                result = result + ch\n",
    "        return result\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['message'] = df['message'].apply(lambda x: space_emoijis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message(i,j):\n",
    "    msg = df.iloc[i:j,df.columns.get_loc('message')].values\n",
    "    assert msg.shape[0] > 0\n",
    "\n",
    "    if (msg.shape[0] > 1):\n",
    "        inpu = msg[0]\n",
    "        for m in msg[1:]:\n",
    "            inpu = \"%s \\n %s\" % (inpu, m)\n",
    "    else:\n",
    "        inpu = msg[0]\n",
    "        \n",
    "    return inpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_message(i, name):\n",
    "    col = df.columns.get_loc('reciever')\n",
    "    \n",
    "    for j in range(1000):\n",
    "        j = i + j\n",
    "        if (name == df.iloc[j,col]):\n",
    "            break    \n",
    "    inpu = get_message(i,j)\n",
    "        \n",
    "    for k in range(1000):\n",
    "        k = j + k\n",
    "        if (name != df.iloc[k,col]):\n",
    "            break\n",
    "    target = get_message(j,k)\n",
    "    \n",
    "    return inpu, target, k-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = []\n",
    "tar = []\n",
    "sen = []\n",
    "rec = []\n",
    "\n",
    "i = 0\n",
    "while i<(len(df)-10):\n",
    "# while i<15:\n",
    "    # get input & target\n",
    "    col = df.columns.get_loc('sender')\n",
    "    name = df.iloc[i,col]\n",
    "    next_names = df.iloc[i+1:i+10,col].values\n",
    "    if (name != \"Christoph Bernkopf\" and name in next_names):\n",
    "        inpu, target, i = get_agg_message(i, name)    \n",
    "        inp.append(inpu)\n",
    "        tar.append(target)\n",
    "        sen.append(name)\n",
    "        rec.append(\"Christoph Bernkopf\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = pd.DataFrame(inp, columns=['input'])\n",
    "dfn['target'] = tar\n",
    "dfn['sender'] = sen\n",
    "dfn['reciever'] = rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little bit of filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfn = dfn[dfn['input'].astype('str').str.len()>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn.to_pickle(os.path.join(data_folder, \"preprocessed_data.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = pd.read_pickle(os.path.join(data_folder, \"preprocessed_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfn['input'].values\n",
    "y = dfn['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tok = Tokenizer(num_words=max_words, filters='\"#$%&()*+,-./;<=>@[\\]^_`{|}~ ')\n",
    "tok.fit_on_texts(np.concatenate([X,y]))\n",
    "\n",
    "def tok_and_pad(arr):\n",
    "    sequences = tok.texts_to_sequences(arr)\n",
    "    return sequences\n",
    "\n",
    "X = tok_and_pad(X)\n",
    "y = tok_and_pad(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((419,), (132,))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = np.array([len(elem) for elem in X])\n",
    "y_lengths = np.array([len(elem) for elem in y])\n",
    "lengths[lengths>50].shape, y_lengths[y_lengths>50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val = 25\n",
    "sns.distplot(lengths[lengths<50],kde=False,\n",
    "             axlabel=\"Port\",label=\"Benign\",color=\"R\", norm_hist=True);\n",
    "plt.legend();\n",
    "plt.title(\"Name\",fontsize=20);\n",
    "# plt.savefig(os.path.join(\"EDA_plots\", col, name))\n",
    "plt.show()\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: classify input by sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = pd.read_pickle(os.path.join(data_folder, \"preprocessed_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfn['input'].values\n",
    "y = dfn['sender'].values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "# y = y.reshape(-1,1)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words, filters='\"#$%&()*+,-./;<=>@[\\]^_`{|}~ ')\n",
    "tok.fit_on_texts(X_train)\n",
    "\n",
    "def tok_and_pad(arr):\n",
    "    sequences = tok.texts_to_sequences(arr)\n",
    "    return sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "train_sequences_matrix = tok_and_pad(X_train)\n",
    "val_sequences_matrix = tok_and_pad(X_val)\n",
    "test_sequences_matrix = tok_and_pad(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok.word_counts\n",
    "names = ['word','count']\n",
    "formats = ['object','f8']\n",
    "dtype = dict(names = names, formats=formats)\n",
    "arr = np.array(list(tok.word_counts.items()), dtype=dtype)\n",
    "arr = pd.DataFrame(arr)\n",
    "arr.sort_values(by=['count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_classifier.fit(train_sequences_matrix, y_train)\n",
    "acc_dummy_classifier = dummy_classifier.score(val_sequences_matrix, y_val)\n",
    "acc_dummy_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, indices = np.unique(y_val, return_inverse=True)\n",
    "most_freq_y_val = u[np.argmax(np.bincount(indices))]\n",
    "le.inverse_transform(most_freq_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(train_sequences_matrix, y_train)\n",
    "tree_score = tree.score(val_sequences_matrix, y_val)\n",
    "tree_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kredy10/simple-lstm-for-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, key_dim=None, **kwargs):\n",
    "        self.key_dim = key_dim\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "         # Weights initializer function\n",
    "        w_initializer = keras.initializers.glorot_uniform()\n",
    "\n",
    "        # Biases initializer function\n",
    "        b_initializer = keras.initializers.Zeros()\n",
    "        \n",
    "        #Matrix to extract the keys\n",
    "        self.key_extract = self.add_weight(name='feature_extract', \n",
    "                                      shape=(int(input_shape[2]),int(self.key_dim)),\n",
    "                                      initializer=w_initializer,\n",
    "                                      trainable=True)\n",
    "        #Key Bias\n",
    "        self.key_bias = self.add_weight(name='feaure_bias', \n",
    "                                      shape=(int(1),int(self.key_dim)),\n",
    "                                      initializer=b_initializer,\n",
    "                                      trainable=True)\n",
    "        \n",
    "        #The Query representing the class\n",
    "        self.Query = self.add_weight(name='Query', \n",
    "                                      shape=(int(self.key_dim),int(1)),\n",
    "                                      initializer=w_initializer,\n",
    "                                      trainable=True)\n",
    "\n",
    "        super(Attention, self).build(input_shape) \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        #Extract the Keys\n",
    "        keys=tf.tensordot(x,self.key_extract,axes=[2,0])+self.key_bias\n",
    "        \n",
    "        #Calculate the similarity between keys and the Query\n",
    "        similar_logits=tf.tensordot(keys,self.Query,axes=[2,0])\n",
    "        \n",
    "        #Normalize it to be between 0 and 1 and sum to 1\n",
    "        attention_weights = tf.nn.sigmoid(similar_logits)\n",
    "        \n",
    "        #Use these Weights to aggregate\n",
    "        weighted_input = tf.matmul(x, attention_weights, transpose_a=True)\n",
    "\n",
    "        return [weighted_input, attention_weights]\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0],input_shape[2],int(1)), (input_shape[0],input_shape[1],1)]\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        base_config['key_dim'] = self.key_dim\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = CuDNNLSTM(64, return_sequences=True)(layer)\n",
    "    layer, attention_weights = Attention(256)(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(len(le.classes_),name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', # RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_sequences_matrix,\n",
    "          y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(val_sequences_matrix, y_val),\n",
    "          callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test accuracy %f\" % accr[1])\n",
    "# not bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain an output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = pd.read_pickle(os.path.join(data_folder, \"preprocessed_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfn['input'].values\n",
    "y = dfn['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10904,), (21808,))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, np.concatenate([X,y]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokeization starts at 1 (0 is not a token key)\n",
    "- Start == -1\n",
    "- End == -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 50\n",
    "tok = Tokenizer(num_words=max_words, filters='\"#$%&()*+,-./;<=>@[\\]^_`{|}~ ')\n",
    "tok.fit_on_texts(np.concatenate([X,y]))\n",
    "\n",
    "def tok_and_pad(arr, flag):\n",
    "    if (flag == \"input\"):\n",
    "        sequences = tok.texts_to_sequences(arr)\n",
    "        return sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "    else:\n",
    "        sequences = tok.texts_to_sequences(arr)\n",
    "        for i in range(len(sequences)):\n",
    "            sequences[i] = np.insert(sequences[i][:48], 0, -1, axis=0)\n",
    "            sequences[i] = np.append(sequences[i], [-2])\n",
    "        return sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "X_train = tok_and_pad(X_train, \"input\")\n",
    "X_test = tok_and_pad(X_test, \"input\")\n",
    "X_val = tok_and_pad(X_val, \"input\")\n",
    "y_train = tok_and_pad(y_train, \"target\")\n",
    "y_test = tok_and_pad(y_test, \"target\")\n",
    "y_val = tok_and_pad(y_val, \"target\")\n",
    "\n",
    "# reverse_word_map = dict(map(reversed, tok.word_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, inputs, targets, batch_size=32, n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        \n",
    "        X = self.inputs[list_IDs_temp]\n",
    "        y = np_utils.to_categorical(self.targets[list_IDs_temp], num_classes=max_words)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'n_classes': 6,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "train_keys = np.arange(y_train.shape[0])\n",
    "val_keys = np.arange(y_val.shape[0])\n",
    "test_keys = np.arange(y_test.shape[0])\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(train_keys, X_train, y_train, **params)\n",
    "validation_generator = DataGenerator(val_keys, X_val, y_val, **params)\n",
    "test_generator = DataGenerator(test_keys, X_test, y_test, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max_words\n",
    "src_txt_length = max_len\n",
    "sum_txt_length = max_len\n",
    "\n",
    "def RNN():\n",
    "    # encoder input model\n",
    "    inputs = Input(shape=(src_txt_length,))\n",
    "    encoder1 = Embedding(vocab_size, 128)(inputs)\n",
    "    encoder2 = CuDNNLSTM(128)(encoder1)\n",
    "    encoder3 = RepeatVector(sum_txt_length)(encoder2)\n",
    "    \n",
    "    # decoder output model\n",
    "    decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "    outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder1)\n",
    "\n",
    "    # tie it together\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    print(\"inputs.shape  \", inputs.shape)\n",
    "    print(\"encoder1.shape\", encoder1.shape)\n",
    "    print(\"encoder2.shape\", encoder2.shape)\n",
    "    print(\"encoder2.shape\", encoder2.shape)\n",
    "    print(\"decoder1.shape\", decoder1.shape)\n",
    "    print(\"outputs.shape \", outputs.shape)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape   (?, 50)\n",
      "encoder1.shape (?, 50, 128)\n",
      "encoder2.shape (?, 128)\n",
      "encoder2.shape (?, 128)\n",
      "decoder1.shape (?, ?, 128)\n",
      "outputs.shape  (?, 50, 20000)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 50, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 128)           131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 50, 20000)         2580000   \n",
      "=================================================================\n",
      "Total params: 5,403,680\n",
      "Trainable params: 5,403,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', # RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "137/137 [==============================] - 61s 449ms/step - loss: 3.3747 - acc: 0.7448 - val_loss: 2.1133 - val_acc: 0.7453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe703cec828>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=training_generator,\n",
    "          validation_data=validation_generator,\n",
    "          epochs=1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred[batch, word_idx, onehot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31431735, 0.02391104, 0.00807253, 0.00719736, 0.00413261],\n",
       "       [0.3143364 , 0.02391174, 0.00807264, 0.00719746, 0.00413263],\n",
       "       [0.31431907, 0.02391112, 0.00807255, 0.00719737, 0.00413261],\n",
       "       [0.3143433 , 0.023912  , 0.00807267, 0.0071975 , 0.00413264],\n",
       "       [0.31433165, 0.02391155, 0.0080726 , 0.00719743, 0.00413262]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0:5,0,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7618257 , 0.02581327, 0.00619482, 0.00533127, 0.00257122],\n",
       "       [0.7618257 , 0.02581327, 0.00619482, 0.00533127, 0.00257122],\n",
       "       [0.7618257 , 0.02581327, 0.00619482, 0.00533127, 0.00257122],\n",
       "       [0.7618257 , 0.02581327, 0.00619482, 0.00533127, 0.00257122],\n",
       "       [0.7618257 , 0.02581327, 0.00619482, 0.00533127, 0.00257122]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0:5,45,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get token key with np.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.6182568e-01, 2.5813272e-02, 6.1948155e-03, ..., 2.0243035e-06,\n",
       "       1.9762900e-02, 1.9717563e-02], dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred[0][10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,   -1,  295,   45,    1,\n",
       "         16, 2038,  391, 1817,    8,   -2], dtype=int32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_train[10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN_UNITS = 50\n",
    "\n",
    "# encoder_inputs = Input(shape=(max_len,), name='encoder_inputs')\n",
    "# encoder_embedding = Embedding(input_dim=max_words, output_dim=HIDDEN_UNITS,\n",
    "#                               input_length=max_len, name='encoder_embedding')\n",
    "# encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "# encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "# encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "# decoder_inputs = Input(shape=(None, max_words), name='decoder_inputs')\n",
    "# decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "# decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "#                                                                  initial_state=encoder_states)\n",
    "# decoder_dense = Dense(units=max_words, activation='softmax', name='decoder_dense')\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
