{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Chris\n",
    "\n",
    "The goal is to create a chatbot, given a variable length input sequence to output a variable length output that sounds like Chris is writing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.5.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "% matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "mpl.rcParams['figure.dpi']= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "txt_files = glob.glob(os.path.join(data_folder, '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_name(name):\n",
    "#     corr = re.sub('[^A-Za-z0-9öüäéè ]+', '', name)\n",
    "#     if (corr[-1] == ' '):\n",
    "#         corr = corr[:-1]\n",
    "#     return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_df_from_txt_file(file_name):\n",
    "#     f = open(file_name)\n",
    "#     file_name = os.path.split(file_name)[1]\n",
    "    \n",
    "#     date = []\n",
    "#     time = []\n",
    "#     sender = []\n",
    "#     reciever = []\n",
    "#     message = []\n",
    "\n",
    "#     partner_name = file_name[18:-4] + \": \"\n",
    "#     chris_name = 'Christoph Bernkopf: '\n",
    "\n",
    "#     i = 0\n",
    "#     for line in f:\n",
    "#         i += 1\n",
    "#         if(len(line) < 2 or \"Nachrichten in diesem Chat sowie Anrufe\" in line):\n",
    "#     #         print(\"skip line\")\n",
    "#             continue\n",
    "#         if(line[15:18] == \" - \" and line[2] == \".\" and line[5] == \".\"):\n",
    "#             date.append(line[:8])\n",
    "#             time.append(line[10:15])\n",
    "#             if (chris_name in line[18:]):\n",
    "#                 sender.append(clean_name(chris_name[:-2]))\n",
    "#                 reciever.append(clean_name(partner_name[:-2]))\n",
    "#             else:\n",
    "#                 reciever.append(clean_name(chris_name[:-2]))\n",
    "#                 sender.append(clean_name(partner_name[:-2]))\n",
    "#             message.append(line[18:].replace(partner_name, '').replace(chris_name, ''))\n",
    "\n",
    "#     df = pd.DataFrame(date, columns=['date'])\n",
    "#     df['time'] = time\n",
    "#     df['sender'] = sender\n",
    "#     df['reciever'] = reciever\n",
    "#     df['message'] = message\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     del df\n",
    "# except:\n",
    "#     None\n",
    "\n",
    "# for file_name in txt_files:\n",
    "#     print(file_name)\n",
    "#     curr_df = get_df_from_txt_file(file_name)\n",
    "#     if ('df' in locals()):\n",
    "#         df = pd.concat([df, curr_df])\n",
    "#     else:\n",
    "#         df = curr_df\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     print(curr_df.shape, df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Messages into Inputs and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_list = ['<Medien ausgeschlossen>', '\\n', 'Martin TPH CERN <3: ', 'Christoph Gerhardus :): ', 'Megan :): ', 'Standort: ']\n",
    "\n",
    "# for item in remove_list:\n",
    "#     df['message'] = df['message'].apply(lambda x: x.replace(item, ''))\n",
    "\n",
    "# # df['message'] = df['message'][df['message'].astype('str').str[:10] != \"Standort: \"]\n",
    "\n",
    "# # get rid of links\n",
    "# df['message'] = df['message'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# # get rid of nan messages\n",
    "# df['message'] = df['message'].apply(lambda x : x if type(x) == str else \"\")\n",
    "\n",
    "# df = df[df['message'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func(x):\n",
    "#     if (type(x) != str):\n",
    "#         print(x)\n",
    "        \n",
    "# df['message'].apply(lambda x: func(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def char_is_emoji(character):\n",
    "#     return character in emoji.UNICODE_EMOJI\n",
    "\n",
    "# def char_is_spe_char(character):\n",
    "#     return character in \"?!.\"\n",
    "\n",
    "# def text_has_emoji(text):\n",
    "#     for character in text:\n",
    "#         if character in emoji.UNICODE_EMOJI:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# def text_has_spe_char(text):\n",
    "#     for character in text:\n",
    "#         if character in \"?!.\":\n",
    "#             return True\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def space_emoijis(x):\n",
    "#     if (text_has_emoji(x) or text_has_spe_char(x)):\n",
    "#         result = ''\n",
    "#         if (len(x) == 1):\n",
    "#             return result\n",
    "#         for ch in x:\n",
    "#             if (char_is_emoji(ch)):\n",
    "#                 result = result + ' ' + ch\n",
    "#             elif (char_is_spe_char(ch)):\n",
    "#                 result = result + ' ' + ch\n",
    "#             else:\n",
    "#                 result = result + ch\n",
    "#         return result\n",
    "#     else:\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['message'] = df['message'].apply(lambda x: space_emoijis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['sender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_message(i,j):\n",
    "#     msg = df.iloc[i:j,df.columns.get_loc('message')].values\n",
    "#     assert msg.shape[0] > 0\n",
    "\n",
    "#     if (msg.shape[0] > 1):\n",
    "#         inpu = msg[0]\n",
    "#         for m in msg[1:]:\n",
    "#             inpu = \"%s \\n %s\" % (inpu, m)\n",
    "#     else:\n",
    "#         inpu = msg[0]\n",
    "        \n",
    "#     return inpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_agg_message(i, name):\n",
    "#     col = df.columns.get_loc('reciever')\n",
    "    \n",
    "#     for j in range(1000):\n",
    "#         j = i + j\n",
    "#         if (name == df.iloc[j,col]):\n",
    "#             break    \n",
    "#     inpu = get_message(i,j)\n",
    "        \n",
    "#     for k in range(1000):\n",
    "#         k = j + k\n",
    "#         if (name != df.iloc[k,col]):\n",
    "#             break\n",
    "#     target = get_message(j,k)\n",
    "    \n",
    "#     return inpu, target, k-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = []\n",
    "# tar = []\n",
    "# sen = []\n",
    "# rec = []\n",
    "\n",
    "# i = 0\n",
    "# while i<(len(df)-10):\n",
    "# # while i<15:\n",
    "#     # get input & target\n",
    "#     col = df.columns.get_loc('sender')\n",
    "#     name = df.iloc[i,col]\n",
    "#     next_names = df.iloc[i+1:i+10,col].values\n",
    "#     if (name != \"Christoph Bernkopf\" and name in next_names):\n",
    "#         inpu, target, i = get_agg_message(i, name)    \n",
    "#         inp.append(inpu)\n",
    "#         tar.append(target)\n",
    "#         sen.append(name)\n",
    "#         rec.append(\"Christoph Bernkopf\")\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfn = pd.DataFrame(inp, columns=['input'])\n",
    "# dfn['target'] = tar\n",
    "# dfn['sender'] = sen\n",
    "# dfn['reciever'] = rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little bit of filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dfn = dfn[dfn['input'].astype('str').str.len()>20]\n",
    "# dfn.head()\n",
    "# dfn.to_pickle(os.path.join(data_folder, \"preprocessed_data.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dfn = pd.read_pickle(os.path.join(data_folder, \"preprocessed_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = dfn['input'].values\n",
    "# y = dfn['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # insert tokenizer and tokenize for EDA\n",
    "\n",
    "# X = tok_and_pad(X)\n",
    "# y = tok_and_pad(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = np.array([len(elem) for elem in X])\n",
    "# y_lengths = np.array([len(elem) for elem in y])\n",
    "# lengths[lengths>50].shape, y_lengths[y_lengths>50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = 25\n",
    "# sns.distplot(lengths[lengths<50],kde=False,\n",
    "#              axlabel=\"Port\",label=\"Benign\",color=\"R\", norm_hist=True);\n",
    "# plt.legend();\n",
    "# plt.title(\"Name\",fontsize=20);\n",
    "# # plt.savefig(os.path.join(\"EDA_plots\", col, name))\n",
    "# plt.show()\n",
    "# # plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: classify input by sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfn = pd.read_pickle(os.path.join(data_folder, \"preprocessed_data.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = dfn['input'].values\n",
    "# y = dfn['sender'].values\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(y)\n",
    "# # y = y.reshape(-1,1)\n",
    "# le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)\n",
    "# X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[:5], y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_words = 20000\n",
    "# max_len = 200\n",
    "# tok = Tokenizer(num_words=max_words, filters='\"#$%&()*+,-./;<=>@[\\]^_`{|}~ ')\n",
    "# tok.fit_on_texts(X_train)\n",
    "\n",
    "# def tok_and_pad(arr):\n",
    "#     sequences = tok.texts_to_sequences(arr)\n",
    "#     return sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "# train_sequences_matrix = tok_and_pad(X_train)\n",
    "# val_sequences_matrix = tok_and_pad(X_val)\n",
    "# test_sequences_matrix = tok_and_pad(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tok.word_counts\n",
    "# names = ['word','count']\n",
    "# formats = ['object','f8']\n",
    "# dtype = dict(names = names, formats=formats)\n",
    "# arr = np.array(list(tok.word_counts.items()), dtype=dtype)\n",
    "# arr = pd.DataFrame(arr)\n",
    "# arr.sort_values(by=['count'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain an output sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = pd.read_pickle(os.path.join(data_folder, \"preprocessed_data.pkl\"))\n",
    "dfn = dfn.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfn['input'].values\n",
    "y = dfn['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10904,), (21808,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, np.concatenate([X,y]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokeization starts at 1 (0 is not a token key)\n",
    "- Start == -1\n",
    "- End == -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['okk sag ma bescheid dann'\n",
      " 'lieferung ist schon da sorry der dude hat mir gestern mitternacht geschrieben']\n"
     ]
    }
   ],
   "source": [
    "input_lang = dfn['input'].apply(lambda x: normalizeString(x)).values\n",
    "output_lang = dfn['target'].apply(lambda x: normalizeString(x)).values\n",
    "pairs = np.stack([input_lang, output_lang]).T\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH #and \\\n",
    "#         p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before: 10904, Length after 7885\n"
     ]
    }
   ],
   "source": [
    "len_bef = len(pairs)\n",
    "pairs = filterPairs(pairs)\n",
    "print(\"Length before: %d, Length after %d\" % (len_bef, len(pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, inputs, targets, batch_size=32, n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        \n",
    "        X = self.inputs[list_IDs_temp]\n",
    "        y = np_utils.to_categorical(self.targets[list_IDs_temp], num_classes=max_words)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'n_classes': 6,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "train_keys = np.arange(y_train.shape[0])\n",
    "val_keys = np.arange(y_val.shape[0])\n",
    "test_keys = np.arange(y_test.shape[0])\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(train_keys, X_train, y_train, **params)\n",
    "validation_generator = DataGenerator(val_keys, X_val, y_val, **params)\n",
    "test_generator = DataGenerator(test_keys, X_test, y_test, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max_words\n",
    "src_txt_length = max_len\n",
    "sum_txt_length = max_len\n",
    "\n",
    "def RNN():\n",
    "    # encoder input model\n",
    "    inputs = Input(shape=(src_txt_length,))\n",
    "    encoder1 = Embedding(vocab_size, 128)(inputs)\n",
    "    encoder2 = CuDNNLSTM(128, return_sequences=False)(encoder1)\n",
    "    encoder3 = RepeatVector(sum_txt_length)(encoder2)\n",
    "    \n",
    "    # decoder output model\n",
    "    decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "    outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder1)\n",
    "\n",
    "    # tie it together\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    print(\"inputs.shape  \", inputs.shape)\n",
    "    print(\"encoder1.shape\", encoder1.shape)\n",
    "    print(\"encoder2.shape\", encoder2.shape)\n",
    "    print(\"encoder2.shape\", encoder2.shape)\n",
    "    print(\"decoder1.shape\", decoder1.shape)\n",
    "    print(\"outputs.shape \", outputs.shape)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', # RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator=training_generator,\n",
    "          validation_data=validation_generator,\n",
    "          epochs=10,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred[batch, word_idx, onehot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0:5,0,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred[0:5,45,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get token key with np.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(pred[0][10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argmax(y_train[10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
